import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.models import Campus, filter_documents_by_campus
from src.document_loader import NoticeDocumentLoader, create_sample_notices


def test_langchain_filtering():
    print("=" * 40)
    print("LangChain ê¸°ë°˜ ìº í¼ìŠ¤ í•„í„°ë§ í…ŒìŠ¤íŠ¸")
    print("=" * 40)

    notices = create_sample_notices()
    loader = NoticeDocumentLoader()
    all_chunks = loader.load_and_split(notices)

    print(f"ğŸ“¢ ì „ì²´ ì²­í¬ ìˆ˜: {len(all_chunks)}")

    test_users = [
        (Campus.CHEONAN, "ì²œì•ˆ ìº í¼ìŠ¤ ì‚¬ìš©ì"),
        (Campus.SINGWAN, "ì‹ ê´€ ìº í¼ìŠ¤ ì‚¬ìš©ì")
    ]

    for user_campus, user_desc in test_users:
        print(f"\nğŸ‘¤ {user_desc} ({user_campus.value})")

        filtered_chunks = filter_documents_by_campus(all_chunks, user_campus)

        print(f"ğŸ” í•„í„°ë§ í›„: {len(filtered_chunks)}ê°œ")
        print("í‘œì‹œë˜ëŠ” ê³µì§€ì‚¬í•­:")
        for chunk in filtered_chunks:
            print(f"  - {chunk.metadata['title']} ({chunk.metadata['campus']})")


def test_langchain_structure():
    print("\n" + "=" * 40)
    print("LangChain Document êµ¬ì¡° í…ŒìŠ¤íŠ¸")
    print("=" * 40)

    notices = create_sample_notices()
    loader = NoticeDocumentLoader()
    chunks = loader.load_and_split(notices)

    sample_chunk = chunks[0]
    print(f"ğŸ“ Document íƒ€ì…: {type(sample_chunk)}")
    print(f"ğŸ“„ page_content ê¸¸ì´: {len(sample_chunk.page_content)}ì")
    print(f"ğŸ·ï¸ metadata í‚¤: {list(sample_chunk.metadata.keys())}")
    print(f"ğŸ“‹ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
    print(f"   {sample_chunk.page_content[:100]}...")


if __name__ == "__main__":
    test_langchain_structure()
    test_langchain_filtering()

    print("\nâœ… LangChain ë¦¬íŒ©í† ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ë‹¤ìŒ ë‹¨ê³„: ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ êµ¬í˜„")
